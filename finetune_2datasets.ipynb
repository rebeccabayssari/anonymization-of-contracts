{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets pandas pyarrow flair torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Combined dataset saved as train_combined.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define the mapping from numerical labels to BIO tags.\n",
    "\n",
    "label_map = {\n",
    "    0: \"O\",  \n",
    "    1: \"B-PER\", 2: \"I-PER\",\n",
    "    3: \"B-ORG\", 4: \"I-ORG\",\n",
    "    5: \"B-LOC\", 6: \"I-LOC\"\n",
    "}\n",
    "\n",
    "def convert_labels(tags):\n",
    "    \"\"\"\n",
    "    Convert a list of tags \n",
    "    into a list of BIO labels using the label_map.\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for t in tags:\n",
    "        try:\n",
    "            # Try to convert the tag to an integer and use the label map.\n",
    "            t_int = int(t)\n",
    "            new_tags.append(label_map.get(t_int, \"O\"))\n",
    "        except ValueError:\n",
    "            new_tags.append(t)\n",
    "    return new_tags\n",
    "\n",
    "\n",
    "# Step 1: Load and process Adminset-NER from Parquet\n",
    "\n",
    "df_adminset = pd.read_parquet(\"adminset_ner.parquet\")\n",
    "# Convert numeric ner_tags to BIO strings\n",
    "df_adminset[\"ner_tags\"] = df_adminset[\"ner_tags\"].apply(convert_labels)\n",
    "\n",
    "\n",
    "# Step 2: Load and process wikiner_fr from Hugging Face\n",
    "#The dataset falls within the 100K - 1M size range / Approximately 170,634 sentences\n",
    "wikiner_dataset = load_dataset(\"Jean-Baptiste/wikiner_fr\", split=\"train\")\n",
    "df_wikiner = pd.DataFrame(wikiner_dataset)\n",
    "# Convert wikiner_fr ner_tags to BIO strings\n",
    "df_wikiner[\"ner_tags\"] = df_wikiner[\"ner_tags\"].apply(convert_labels)\n",
    "\n",
    "#combine the 2\n",
    "combined_df = pd.concat([\n",
    "    df_adminset[[\"tokens\", \"ner_tags\"]],\n",
    "    df_wikiner[[\"tokens\", \"ner_tags\"]]\n",
    "], ignore_index=True)\n",
    "\n",
    "\n",
    "# Step 4: Save combined dataset in Flair train format\n",
    "\n",
    "output_filename = \"train_combined.txt\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for tokens, tags in zip(combined_df[\"tokens\"], combined_df[\"ner_tags\"]):\n",
    "        for word, tag in zip(tokens, tags):\n",
    "            f.write(f\"{word} {tag}\\n\")\n",
    "        f.write(\"\\n\")  # Separate sentences by an empty line\n",
    "\n",
    "print(f\" Combined dataset saved as {output_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-26 16:31:22,330 Reading data from .\n",
      "2025-03-26 16:31:22,331 Train: train_combined.txt\n",
      "2025-03-26 16:31:22,332 Dev: None\n",
      "2025-03-26 16:31:22,332 Test: None\n",
      "2025-03-26 16:31:51,862 No test split found. Using 10% (i.e. 12141 samples) of the train split as test data\n",
      "2025-03-26 16:31:51,939 No dev split found. Using 10% (i.e. 10927 samples) of the train split as dev data\n",
      "2025-03-26 16:31:57,747 SequenceTagger predicts: Dictionary with 19 tags: O, S-LOC, B-LOC, E-LOC, I-LOC, S-PER, B-PER, E-PER, I-PER, S-MISC, B-MISC, E-MISC, I-MISC, S-ORG, B-ORG, E-ORG, I-ORG, <START>, <STOP>\n",
      "2025-03-26 16:31:58,153 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,155 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      '0-/vol/home-vol2/ml/akbikala/.flair/embeddings/fr-wiki-fasttext-300d-1M'\n",
      "      (embedding): Embedding(1000000, 300)\n",
      "    )\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.5, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 1024)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_2): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.5, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 1024)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=2348, out_features=2348, bias=True)\n",
      "  (rnn): LSTM(2348, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=19, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2025-03-26 16:31:58,156 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,157 Corpus: 98343 train + 10927 dev + 12141 test sentences\n",
      "2025-03-26 16:31:58,157 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,158 Train:  98343 sentences\n",
      "2025-03-26 16:31:58,159         (train_with_dev=False, train_with_test=False)\n",
      "2025-03-26 16:31:58,160 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,161 Training Params:\n",
      "2025-03-26 16:31:58,162  - learning_rate: \"0.05\" \n",
      "2025-03-26 16:31:58,164  - mini_batch_size: \"6\"\n",
      "2025-03-26 16:31:58,165  - max_epochs: \"5\"\n",
      "2025-03-26 16:31:58,167  - shuffle: \"True\"\n",
      "2025-03-26 16:31:58,168 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,169 Plugins:\n",
      "2025-03-26 16:31:58,170  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
      "2025-03-26 16:31:58,171 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,172 Final evaluation on model from best epoch (best-model.pt)\n",
      "2025-03-26 16:31:58,174  - metric: \"('micro avg', 'f1-score')\"\n",
      "2025-03-26 16:31:58,175 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,176 Computation:\n",
      "2025-03-26 16:31:58,177  - compute on device: cpu\n",
      "2025-03-26 16:31:58,179  - embedding storage: cpu\n",
      "2025-03-26 16:31:58,181 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,182 Model training base path: \"flair_output_combined\"\n",
      "2025-03-26 16:31:58,183 ----------------------------------------------------------------------------------------------------\n",
      "2025-03-26 16:31:58,184 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flair\\trainers\\trainer.py:545: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-26 16:48:04,262 epoch 1 - iter 1639/16391 - loss 0.10721415 - time (sec): 966.06 - samples/sec: 268.28 - lr: 0.050000 - momentum: 0.000000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from flair.datasets import ColumnCorpus\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# Load dataset\n",
    "corpus = ColumnCorpus('.', columns, train_file='train_combined.txt')\n",
    "\n",
    "tagger = SequenceTagger.load(\"flair/ner-french\")\n",
    "\n",
    "# at first i tried minibatchsize= 16 but my kernel crahsed since im on cpu so i used minibatchsize= 6\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "trainer.train(\"flair_output_combined\",\n",
    "              learning_rate=0.05,\n",
    "              mini_batch_size=6,\n",
    "              max_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Combined spaCy training file created at C:\\Users\\hp\\Downloads\\clever contact\\combined_all.spacy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Define a label mapping from numerical tags to BIO tags.\n",
    "label_map = {\n",
    "    0: \"O\",  \n",
    "    1: \"B-PER\", 2: \"I-PER\",\n",
    "    3: \"B-ORG\", 4: \"I-ORG\",\n",
    "    5: \"B-LOC\", 6: \"I-LOC\"\n",
    "}\n",
    "\n",
    "# convert a list of tags (which may be numbers or strings) to BIO strings.\n",
    "def convert_labels(tags):\n",
    "    new_tags = []\n",
    "    for t in tags:\n",
    "        try:\n",
    "            # If t is a number, convert it to int and then map\n",
    "            new_tags.append(label_map[int(t)])\n",
    "        except ValueError:\n",
    "            # Otherwise, assume it's already a string (like \"O\") and keep it.\n",
    "            new_tags.append(t)\n",
    "    return new_tags\n",
    "\n",
    "\n",
    "adminset_dataset = load_dataset(\"taln-ls2n/Adminset-NER\")['train']\n",
    "df_adminset = pd.DataFrame(adminset_dataset)\n",
    "df_adminset['ner_tags'] = df_adminset['ner_tags'].apply(convert_labels)\n",
    "\n",
    "\n",
    "wikiner_dataset = load_dataset(\"Jean-Baptiste/wikiner_fr\", split=\"train\")\n",
    "df_wikiner = pd.DataFrame(wikiner_dataset)\n",
    "df_wikiner['ner_tags'] = df_wikiner['ner_tags'].apply(convert_labels)\n",
    "\n",
    "\n",
    "combined_df = pd.concat([\n",
    "    df_adminset[['tokens', 'ner_tags']],\n",
    "    df_wikiner[['tokens', 'ner_tags']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Define a function to create a spaCy Doc with entities from tokens and BIO tags.\n",
    "nlp = spacy.blank(\"fr\")\n",
    "\n",
    "def create_spacy_doc(tokens, ner_tags):\n",
    "    # Create a doc from the tokens (join them with spaces)\n",
    "    doc = nlp.make_doc(\" \".join(tokens))\n",
    "    ents = []\n",
    "    current_ent = None\n",
    "    offset = 0\n",
    "    # Iterate over each token and its corresponding tag\n",
    "    for token, tag_label in zip(tokens, ner_tags):\n",
    "        token_len = len(token)\n",
    "        if tag_label.startswith(\"B-\"):\n",
    "            if current_ent:\n",
    "                ents.append(current_ent)\n",
    "            current_ent = [offset, offset + token_len, tag_label[2:]]  # remove \"B-\"\n",
    "        elif tag_label.startswith(\"I-\") and current_ent and current_ent[2] == tag_label[2:]:\n",
    "            current_ent[1] = offset + token_len\n",
    "        else:\n",
    "            if current_ent:\n",
    "                ents.append(current_ent)\n",
    "                current_ent = None\n",
    "        offset += token_len + 1  # account for the space\n",
    "    if current_ent:\n",
    "        ents.append(current_ent)\n",
    "    \n",
    "    # Convert the collected entity spans into spaCy spans\n",
    "    spacy_ents = []\n",
    "    for start, end, label in ents:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='expand')\n",
    "        if span is not None:\n",
    "            spacy_ents.append(span)\n",
    "    doc.ents = spacy_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Function to convert a DataFrame into a DocBin file\n",
    "def convert_dataframe_to_docbin(df, output_path):\n",
    "    db = DocBin()\n",
    "    for _, row in df.iterrows():\n",
    "        doc = create_spacy_doc(row['tokens'], row['ner_tags'])\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)\n",
    "    print(f\" Combined spaCy training file created at {output_path}\")\n",
    "\n",
    "output_dir = r\"C:\\Users\\hp\\Downloads\\clever contact\"\n",
    "output_path = os.path.join(output_dir, \"combined_all.spacy\")\n",
    "\n",
    "# Convert the combined DataFrame to spaCy's binary format\n",
    "convert_dataframe_to_docbin(combined_df, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training set with 109269 docs to C:\\Users\\hp\\Downloads\\clever contact\\combined_train.spacy\n",
      "Saved dev set with 12142 docs to C:\\Users\\hp\\Downloads\\clever contact\\combined_dev.spacy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "combined_all_path = r\"C:\\Users\\hp\\Downloads\\clever contact\\combined_all.spacy\"\n",
    "\n",
    "# Load the combined dataset using a blank French model.\n",
    "nlp = spacy.blank(\"fr\")\n",
    "db = DocBin().from_disk(combined_all_path)\n",
    "docs = list(db.get_docs(nlp.vocab))\n",
    "\n",
    "# Shuffle the docs to randomize the split.\n",
    "random.shuffle(docs)\n",
    "\n",
    "# Split ratio: 90% for training, 10% for dev.\n",
    "split_idx = int(0.9 * len(docs))\n",
    "train_docs = docs[:split_idx]\n",
    "dev_docs = docs[split_idx:]\n",
    "\n",
    "# Create new DocBin objects for training and dev.\n",
    "train_db = DocBin(docs=train_docs)\n",
    "dev_db = DocBin(docs=dev_docs)\n",
    "\n",
    "# Define output paths.\n",
    "output_dir = r\"C:\\Users\\hp\\Downloads\\clever contact\"\n",
    "train_output_path = os.path.join(output_dir, \"combined_train.spacy\")\n",
    "dev_output_path = os.path.join(output_dir, \"combined_dev.spacy\")\n",
    "\n",
    "# Save the split datasets.\n",
    "train_db.to_disk(train_output_path)\n",
    "dev_db.to_disk(dev_output_path)\n",
    "\n",
    "print(f\"Saved training set with {len(train_docs)} docs to {train_output_path}\")\n",
    "print(f\"Saved dev set with {len(dev_docs)} docs to {dev_output_path}\")\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: python -m spacy init config [OPTIONS] OUTPUT_FILE\n",
      "Try 'python -m spacy init config --help' for help.\n",
      "┌─ Error ─────────────────────────────────────────────────────────────────────┐\n",
      "│ No such option: --pretrained-model Did you mean --pretraining?              │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy init config config.cfg --lang fr --pipeline ner --optimize accuracy --pretrained-model fr_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: fr\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spacyconfig.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train spacyconfig.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy init config spacyconfig.cfg --lang fr --pipeline ner --optimize accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = r\"\"\"\n",
    "[paths]\n",
    "train = \"C:\\Users\\hp\\Downloads\\clever contact\\combined_train.spacy\"\n",
    "dev = \"C:\\Users\\hp\\Downloads\\clever contact\\combined_dev.spacy\"\n",
    "\n",
    "[system]\n",
    "gpu_allocator = null\n",
    "seed = 42\n",
    "\n",
    "[nlp]\n",
    "lang = \"fr\"\n",
    "pipeline = [\"tok2vec\", \"ner\"]\n",
    "batch_size = 100\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\n",
    "[components.tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[initialize]\n",
    "vectors = null\n",
    "init_tok2vec = \"fr_core_news_sm/tok2vec\"\n",
    "\n",
    "[training]\n",
    "seed = ${system.seed}\n",
    "gpu_allocator = ${system.gpu_allocator}\n",
    "max_epochs = 10\n",
    "patience = 1000\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "learn_rate = 0.001\n",
    "L2 = 0.01\n",
    "grad_clip = 1.0\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: spacy_my_finetuned_model\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     43.21    0.00    0.00    0.00    0.00\n",
      "  0     200          0.00   2335.84   46.33   54.16   40.48    0.46\n",
      "  0     400          0.00   1822.37   48.14   69.52   36.82    0.48\n",
      "  0     600          0.00   2145.25   56.31   67.61   48.24    0.56\n",
      "  0     800          0.00   2436.33   58.49   72.84   48.86    0.58\n",
      "  0    1000          0.00   2779.94   63.87   73.69   56.36    0.64\n",
      "  0    1200          0.00   3068.44   67.02   67.50   66.54    0.67\n",
      "  0    1400          0.00   3547.89   68.29   73.13   64.05    0.68\n",
      "  0    1600          0.00   4367.91   71.14   76.68   66.35    0.71\n",
      "  0    1800          0.00   5069.25   72.92   74.76   71.18    0.73\n",
      "  0    2000          0.00   5846.11   73.55   83.09   65.98    0.74\n",
      "  0    2200          0.00   6975.74   76.15   78.89   73.59    0.76\n",
      "  0    2400          0.00   7975.11   76.85   79.28   74.56    0.77\n",
      "  0    2600          0.00   7965.84   77.26   82.73   72.46    0.77\n",
      "  0    2800          0.00   7648.11   78.29   84.68   72.80    0.78\n",
      "  0    3000          0.00   7608.25   79.06   80.17   77.97    0.79\n",
      "  0    3200          0.00   7074.31   79.06   85.80   73.30    0.79\n",
      "  0    3400          0.00   7148.28   78.99   83.99   74.56    0.79\n",
      "  0    3600          0.00   6842.92   80.24   84.11   76.71    0.80\n",
      "  0    3800          0.00   6716.57   79.88   86.32   74.34    0.80\n",
      "  0    4000          0.00   6462.70   80.75   82.28   79.28    0.81\n",
      "  0    4200          0.00   6821.10   80.98   84.03   78.14    0.81\n",
      "  0    4400          0.00   6427.11   81.94   83.91   80.07    0.82\n",
      "  1    4600          0.00   6028.59   82.26   85.05   79.65    0.82\n",
      "  1    4800          0.00   5506.29   82.29   83.47   81.14    0.82\n",
      "  1    5000          0.00   5533.29   82.68   85.83   79.75    0.83\n",
      "  1    5200          0.00   5732.32   82.28   86.55   78.41    0.82\n",
      "  1    5400          0.00   5562.56   82.93   83.22   82.65    0.83\n",
      "  1    5600          0.00   5565.67   82.39   87.90   77.54    0.82\n",
      "  1    5800          0.00   5587.84   83.02   86.20   80.06    0.83\n",
      "  1    6000          0.00   5532.07   82.98   85.43   80.66    0.83\n",
      "  1    6200          0.00   5450.78   83.57   85.26   81.94    0.84\n",
      "  1    6400          0.00   5327.77   83.47   86.04   81.06    0.83\n",
      "  1    6600          0.00   5356.42   83.27   86.74   80.06    0.83\n",
      "  1    6800          0.00   5302.10   83.60   87.69   79.87    0.84\n",
      "  1    7000          0.00   5595.56   83.23   86.33   80.33    0.83\n",
      "  1    7200          0.00   5353.66   83.33   89.35   78.08    0.83\n",
      "  1    7400          0.00   5135.41   83.90   88.52   79.74    0.84\n",
      "  1    7600          0.00   5702.14   84.56   87.38   81.92    0.85\n",
      "  2    7800          0.00   4457.23   83.84   85.59   82.16    0.84\n",
      "  2    8000          0.00   4372.68   84.70   86.84   82.66    0.85\n",
      "  2    8200          0.00   4575.57   84.47   86.45   82.58    0.84\n",
      "  2    8400          0.00   4590.57   84.66   87.25   82.22    0.85\n",
      "  2    8600          0.00   4775.29   84.52   87.16   82.04    0.85\n",
      "  2    8800          0.00   4769.72   84.56   86.80   82.43    0.85\n",
      "  2    9000          0.00   4462.45   84.23   89.27   79.73    0.84\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "spacy_my_finetuned_model\\model-last\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli.train import train\n",
    "\n",
    "# Write your configuration to a file (your config.cfg as above)\n",
    "with open(\"spacyconfig.cfg\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(config_str)\n",
    "\n",
    "# Now, run training with overrides for the training and dev paths,\n",
    "# and also override max_epochs if needed:\n",
    "train(\"spacyconfig.cfg\", output_path=\"spacy_my_finetuned_model\",\n",
    "      overrides={\n",
    "          \"paths.train\": r\"C:\\Users\\hp\\Downloads\\clever contact\\combined_train.spacy\",\n",
    "          \"paths.dev\": r\"C:\\Users\\hp\\Downloads\\clever contact\\combined_dev.spacy\",\n",
    "          \"training.max_epochs\": 10  # override max_epochs to a positive number\n",
    "      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_1868\\611305634.py:5: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  nlp_trained = spacy.load('spacy_my_finetuned_model\\model-best')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracted entities saved to predicted_entitiees_spacy_finetuned_with2datasets4.json!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "# Loading your fine-tuned spaCy model \n",
    "nlp_trained = spacy.load('spacy_my_finetuned_model\\model-best')\n",
    "\n",
    "#file_path = r\"C:\\Users\\hp\\Downloads\\clever contact\\cleanedcontract2.txt\"\n",
    "file_path = r\"C:\\Users\\hp\\Downloads\\clever contact\\cleanedcontract4.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    contract_text = f.read()\n",
    "\n",
    "# Process the text to get a spaCy Doc with entities\n",
    "doc = nlp_trained(contract_text)\n",
    "\n",
    "# Extract entities into a list of dictionaries\n",
    "extracted_entities = []\n",
    "for ent in doc.ents:\n",
    "    extracted_entities.append({\n",
    "        \"text\": ent.text,\n",
    "        \"type\": ent.label_,\n",
    "        \"start\": ent.start_char,\n",
    "        \"end\": ent.end_char\n",
    "    })\n",
    "\n",
    "\n",
    "output_file = \"predicted_entitiees_spacy_finetuned_with2datasets4.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extracted_entities, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\" Extracted entities saved to {output_file}!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
